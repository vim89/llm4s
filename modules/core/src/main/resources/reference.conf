# Unified llm4s configuration namespace
llm4s {
  # Primary model selection, e.g. "openai/gpt-4o", "anthropic/claude-3-7-sonnet-latest", "ollama/llama3.1"
  llm {
    model = ${?LLM_MODEL}
  }

  # OpenAI (also used for OpenRouter via baseUrl)
  openai {
    # Safe default; can be overridden by env or -D
    baseUrl = "https://api.openai.com/v1"
    baseUrl = ${?OPENAI_BASE_URL}
    apiKey  = ${?OPENAI_API_KEY}
    organization = ${?OPENAI_ORGANIZATION}
  }

  # Azure OpenAI
  azure {
    endpoint   = ${?AZURE_API_BASE}
    apiKey     = ${?AZURE_API_KEY}
    apiVersion = ${?AZURE_API_VERSION}
  }

  # Anthropic
  anthropic {
    baseUrl = "https://api.anthropic.com"
    baseUrl = ${?ANTHROPIC_BASE_URL}
    apiKey  = ${?ANTHROPIC_API_KEY}
  }

  # Ollama (local models)
  ollama {
    baseUrl = ${?OLLAMA_BASE_URL}
  }

  # Tracing: Langfuse (app/runners may use these)
  tracing {
    # Tracing mode: langfuse | console | noop
    mode = "console"
    mode = ${?TRACING_MODE}
    langfuse {
      url       = ${?LANGFUSE_URL}
      publicKey = ${?LANGFUSE_PUBLIC_KEY}
      secretKey = ${?LANGFUSE_SECRET_KEY}
      env       = ${?LANGFUSE_ENV}
      release   = ${?LANGFUSE_RELEASE}
      version   = ${?LANGFUSE_VERSION}
    }
  }

  # Embeddings configuration
  embeddings {
    # Unified format: provider/model (e.g., "openai/text-embedding-3-small")
    # Takes precedence over legacy provider + model settings
    model = ${?EMBEDDING_MODEL}

    # Legacy fallback (used if model is not set)
    provider   = ${?EMBEDDING_PROVIDER}
    inputPath  = ${?EMBEDDING_INPUT_PATH}
    inputPaths = ${?EMBEDDING_INPUT_PATHS}
    query      = ${?EMBEDDING_QUERY}

    openai {
      # Default base URL used when EMBEDDING_MODEL=openai/... is set
      baseUrl = "https://api.openai.com/v1"
      baseUrl = ${?OPENAI_EMBEDDING_BASE_URL}
      model   = ${?OPENAI_EMBEDDING_MODEL}
    }

    voyage {
      apiKey  = ${?VOYAGE_API_KEY}
      # Default base URL used when EMBEDDING_MODEL=voyage/... is set
      baseUrl = "https://api.voyageai.com/v1"
      baseUrl = ${?VOYAGE_EMBEDDING_BASE_URL}
      model   = ${?VOYAGE_EMBEDDING_MODEL}
    }

    # Ollama embeddings (local)
    # Env: OLLAMA_EMBEDDING_BASE_URL (default: http://localhost:11434), OLLAMA_EMBEDDING_MODEL
    ollama {
      baseUrl = "http://localhost:11434"
      baseUrl = ${?OLLAMA_EMBEDDING_BASE_URL}
      model = "nomic-embed-text"
      model = ${?OLLAMA_EMBEDDING_MODEL}
      apiKey = "not-required"
    }

    chunking {
      size    = ${?CHUNK_SIZE}
      overlap = ${?CHUNK_OVERLAP}
      enabled = ${?CHUNKING_ENABLED}
    }

    # Embeddings UI configuration (used by samples and tools).
    # These can also be controlled via env vars:
    #   MAX_ROWS_PER_FILE, TOP_DIMS_PER_ROW, GLOBAL_TOPK, SHOW_GLOBAL_TOP, COLOR, TABLE_WIDTH
    ui {
      maxRowsPerFile = 200
      maxRowsPerFile = ${?MAX_ROWS_PER_FILE}
      topDimsPerRow  = 6
      topDimsPerRow  = ${?TOP_DIMS_PER_ROW}
      globalTopK     = 10
      globalTopK     = ${?GLOBAL_TOPK}
      showGlobalTop  = false
      showGlobalTop  = ${?SHOW_GLOBAL_TOP}
      colorEnabled   = true
      colorEnabled   = ${?COLOR}
      tableWidth     = 120
      tableWidth     = ${?TABLE_WIDTH}
    }

    # Experimental stubs toggle for non-text embeddings (used by UniversalEncoder).
    # Can also be controlled via ENABLE_EXPERIMENTAL_STUBS env var.
    experimentalStubs = ${?ENABLE_EXPERIMENTAL_STUBS}

    # Local/stub model names used for non-text modalities.
    # These are only used when experimental stubs are enabled.
    localModels {
      imageModel = "openclip-vit-b32"
      imageModel = ${?LLM4S_EMBEDDINGS_LOCAL_IMAGE_MODEL}
      audioModel = "wav2vec2-base"
      audioModel = ${?LLM4S_EMBEDDINGS_LOCAL_AUDIO_MODEL}
      videoModel = "timesformer-base"
      videoModel = ${?LLM4S_EMBEDDINGS_LOCAL_VIDEO_MODEL}
    }
  }

  # Runner-only convenience (apps may use):
  workspace {
    # Workspace directory for codegen tools (defaults to ~/code-workspace)
    dir = ${?WORKSPACE_DIR}

    # Workspace Docker image (defaults to WorkspaceSettings.DefaultImage)
    image = ${?WORKSPACE_IMAGE}

    # Workspace service port (defaults to WorkspaceSettings.DefaultPort)
    port = ${?WORKSPACE_PORT}

    # Trace log output path (defaults to <dir>/log/codegen-trace.md)
    traceLogPath = ${?WORKSPACE_TRACE_LOG}

    # Legacy override (ignored by PureConfig loader; keep comment for reference)
    # path = ${?WORKSPACE_PATH}
  }

  # Optional model metadata overrides.
  # If set, applications can read this path (via Llm4sConfig.modelMetadataOverridePath)
  # and pass it explicitly to ModelRegistry.loadCustomMetadata(path).
  modelMetadata {
    file = ${?LLM4S_MODEL_METADATA_FILE}
  }

  # Reranker configuration (load at the app edge and pass to RerankerFactory.fromConfig).
  rerank {
    provider = ${?RERANK_PROVIDER} # cohere | none
    cohere {
      apiKey  = ${?COHERE_API_KEY}
      baseUrl = ${?COHERE_RERANK_BASE_URL}
      model   = ${?COHERE_RERANK_MODEL}
    }
  }

  # PostgreSQL settings used by permission-aware RAG (loaded at the app edge).
  # Env var overrides supported for convenience:
  #   PGVECTOR_HOST, PGVECTOR_PORT, PGVECTOR_DATABASE,
  #   PGVECTOR_USER, PGVECTOR_PASSWORD,
  #   PGVECTOR_TABLE, PGVECTOR_MAX_POOL_SIZE
  rag {
    permissions {
      pg {
        host = "localhost"
        host = ${?PGVECTOR_HOST}
        port = 5432
        port = ${?PGVECTOR_PORT}
        database = "postgres"
        database = ${?PGVECTOR_DATABASE}

        user = "postgres"
        user = ${?PGVECTOR_USER}
        password = ""
        password = ${?PGVECTOR_PASSWORD}

        vectorTableName = "vectors"
        vectorTableName = ${?PGVECTOR_TABLE}

        maxPoolSize = 10
        maxPoolSize = ${?PGVECTOR_MAX_POOL_SIZE}
      }
    }
  }
}
