package org.llm4s.llmconnect.provider

import com.azure.ai.openai.models._
import com.azure.ai.openai.{ OpenAIClientBuilder, OpenAIServiceVersion, OpenAIClient => AzureOpenAIClient }
import com.azure.core.credential.{ AzureKeyCredential, KeyCredential }
import org.llm4s.error.ThrowableOps._
import org.llm4s.llmconnect.LLMClient
import org.llm4s.llmconnect.config.{ AzureConfig, OpenAIConfig, ProviderConfig }
import org.llm4s.llmconnect.model._
import org.llm4s.llmconnect.streaming._
import org.llm4s.model.TransformationResult
import org.llm4s.toolapi.{ AzureToolHelper, ToolRegistry }
import org.llm4s.types.Result

import scala.jdk.CollectionConverters._
import scala.util.Try

/**
 * LLMClient implementation supporting both OpenAI and Azure OpenAI services.
 *
 * Provides a unified interface for interacting with OpenAI's API and Azure's OpenAI service.
 * Handles message conversion between llm4s format and OpenAI format, completion requests,
 * streaming responses, and tool calling (function calling) capabilities.
 *
 * Uses Azure's OpenAI client library internally, which supports both direct OpenAI and
 * Azure-hosted OpenAI endpoints.
 *
 * == Extended Thinking / Reasoning Support ==
 *
 * For OpenAI o1/o3/o4 models with reasoning capabilities, use [[OpenRouterClient]] instead,
 * which fully supports the `reasoning_effort` parameter. The Azure SDK used by this client
 * does not yet expose the `reasoning_effort` API parameter.
 *
 * For Anthropic Claude models with extended thinking, use [[AnthropicClient]] which has
 * full support for the `thinking` parameter with `budget_tokens`.
 *
 * @param model the model identifier (e.g., "gpt-4", "gpt-3.5-turbo")
 * @param client configured Azure OpenAI client instance
 * @param config provider configuration containing context window and reserve completion settings
 */
class OpenAIClient private (
  private val model: String,
  private val client: AzureOpenAIClient,
  private val config: ProviderConfig
) extends LLMClient {

  /**
   * Creates an OpenAI client for direct OpenAI API access.
   *
   * @param config OpenAI configuration with API key and base URL
   */
  def this(config: OpenAIConfig) = this(
    config.model,
    new OpenAIClientBuilder()
      .credential(new KeyCredential(config.apiKey))
      .endpoint(config.baseUrl)
      .buildClient(),
    config
  )

  /**
   * Creates an OpenAI client for Azure OpenAI service.
   *
   * @param config Azure configuration with API key, endpoint, and API version
   */
  def this(config: AzureConfig) = this(
    config.model,
    new OpenAIClientBuilder()
      .credential(new AzureKeyCredential(config.apiKey))
      .endpoint(config.endpoint)
      .serviceVersion(OpenAIServiceVersion.valueOf(config.apiVersion))
      .buildClient(),
    config
  )

  override def complete(
    conversation: Conversation,
    options: CompletionOptions
  ): Result[Completion] =
    // Transform options and messages for model-specific constraints
    for {
      transformed <- TransformationResult.transform(
        model,
        options,
        conversation.messages,
        dropUnsupported = true
      )
      transformedConversation = conversation.copy(messages = transformed.messages)
      chatOptions             = prepareChatOptions(transformedConversation, transformed.options)
      completions <- Try(client.getChatCompletions(model, chatOptions)).toEither.left.map(e => e.toLLMError)
    } yield convertFromOpenAIFormat(completions)

  override def streamComplete(
    conversation: Conversation,
    options: CompletionOptions = CompletionOptions(),
    onChunk: StreamedChunk => Unit
  ): Result[Completion] =
    // Transform options and messages for model-specific constraints
    TransformationResult.transform(model, options, conversation.messages, dropUnsupported = true).flatMap {
      transformed =>
        val transformedConversation = conversation.copy(messages = transformed.messages)
        val chatOptions             = prepareChatOptions(transformedConversation, transformed.options)

        // Create accumulator for building the final completion
        val accumulator = StreamingAccumulator.create()

        // Check if this model requires fake streaming
        if (transformed.requiresFakeStreaming) {
          // For models that don't support native streaming (e.g., O-series),
          // make a regular completion call and emit the result as a single chunk
          val attempt = Try(client.getChatCompletions(model, chatOptions)).toEither.left.map(e => e.toLLMError)
          attempt.flatMap { completions =>
            val completion = convertFromOpenAIFormat(completions)
            val contentOpt = if (completion.content.nonEmpty) Some(completion.content) else None
            completion.toolCalls.headOption match {
              case Some(first) =>
                val chunk = StreamedChunk(
                  id = completion.id,
                  content = contentOpt,
                  toolCall = Some(first),
                  finishReason = Some("stop")
                )
                onChunk(chunk)
                completion.toolCalls.drop(1).foreach { tc =>
                  onChunk(StreamedChunk(id = completion.id, content = None, toolCall = Some(tc), finishReason = None))
                }
              case None =>
                val chunk = StreamedChunk(
                  id = completion.id,
                  content = contentOpt,
                  toolCall = None,
                  finishReason = Some("stop")
                )
                onChunk(chunk)
            }
            Right(completion)
          }
        } else {
          // Normal streaming path
          val attempt = Try {
            val stream = client.getChatCompletionsStream(model, chatOptions)

            stream.forEach { chatCompletions =>
              if (chatCompletions.getChoices != null && !chatCompletions.getChoices.isEmpty) {
                val choice = chatCompletions.getChoices.get(0)
                val delta  = choice.getDelta

                val toolCalls    = extractStreamingToolCalls(delta)
                val contentOpt   = Option(delta.getContent)
                val finishReason = Option(choice.getFinishReason).map(_.toString)
                val chunkId      = Option(chatCompletions.getId).getOrElse("")

                if (toolCalls.isEmpty) {
                  val chunk = StreamedChunk(
                    id = chunkId,
                    content = contentOpt,
                    toolCall = None,
                    finishReason = finishReason
                  )
                  accumulator.addChunk(chunk)
                  onChunk(chunk)
                } else {
                  val firstChunk = StreamedChunk(
                    id = chunkId,
                    content = contentOpt,
                    toolCall = Some(toolCalls.head),
                    finishReason = finishReason
                  )
                  accumulator.addChunk(firstChunk)
                  onChunk(firstChunk)
                  toolCalls.drop(1).foreach { tc =>
                    val extra = StreamedChunk(
                      id = chunkId,
                      content = None,
                      toolCall = Some(tc),
                      finishReason = None
                    )
                    accumulator.addChunk(extra)
                    onChunk(extra)
                  }
                }

                // Check if streaming is complete
                if (choice.getFinishReason != null) {
                  // Extract usage if available
                  Option(chatCompletions.getUsage).foreach { usage =>
                    accumulator.updateTokens(usage.getPromptTokens, usage.getCompletionTokens)
                  }
                }
              }
            }
          }.toEither.left.map(e => e.toLLMError)

          // Convert accumulated chunks into a final Completion on success
          attempt.flatMap(_ => accumulator.toCompletion.map(c => c.copy(model = model)))
        }
    }

  override def getContextWindow(): Int = config.contextWindow

  override def getReserveCompletion(): Int = config.reserveCompletion

  /**
   * Prepares ChatCompletionsOptions from conversation and completion options.
   *
   * Converts llm4s conversation format to OpenAI ChatCompletionsOptions, applying temperature,
   * token limits, penalties, and tools. Shared between complete() and streamComplete().
   *
   * @param conversation llm4s conversation to convert
   * @param options completion options to apply
   * @return configured ChatCompletionsOptions ready for API call
   */
  private def prepareChatOptions(conversation: Conversation, options: CompletionOptions): ChatCompletionsOptions = {
    // Convert conversation to Azure format
    val chatMessages = convertToOpenAIMessages(conversation)

    // Create chat options
    val chatOptions = new ChatCompletionsOptions(chatMessages)

    // Set options
    chatOptions.setTemperature(options.temperature.doubleValue())
    options.maxTokens.foreach(mt => chatOptions.setMaxTokens(mt))
    chatOptions.setPresencePenalty(options.presencePenalty.doubleValue())
    chatOptions.setFrequencyPenalty(options.frequencyPenalty.doubleValue())
    chatOptions.setTopP(options.topP.doubleValue())

    // Add tools if specified
    if (options.tools.nonEmpty) {
      val toolRegistry = new ToolRegistry(options.tools)
      AzureToolHelper.addToolsToOptions(toolRegistry, chatOptions)
    }

    chatOptions
  }

  /**
   * Extracts tool call information from a streaming response delta.
   *
   * Parses the first tool call from the delta message, converting function call details
   * into llm4s ToolCall format. Used during streaming to capture tool calling requests.
   *
   * @param delta streaming response message delta from OpenAI
   * @return Some(ToolCall) if a function tool call is present, None otherwise
   */
  private def extractStreamingToolCalls(delta: ChatResponseMessage): Seq[ToolCall] =
    Option(delta.getToolCalls)
      .map(
        _.asScala.toSeq.collect { case ftc: ChatCompletionsFunctionToolCall =>
          val function = Option(ftc.getFunction)
          val rawArgs  = function.flatMap(f => Option(f.getArguments)).getOrElse("")
          ToolCall(
            id = ftc.getId,
            name = function.map(_.getName).getOrElse(""),
            arguments = parseStreamingArguments(rawArgs)
          )
        }
      )
      .getOrElse(Seq.empty)

  private def parseStreamingArguments(raw: String): ujson.Value =
    if (raw.isEmpty) ujson.Null else Try(ujson.read(raw)).getOrElse(ujson.Str(raw))

  /**
   * Converts llm4s Conversation to OpenAI ChatRequestMessage format.
   *
   * Transforms each message type (User, System, Assistant, Tool) into the corresponding
   * OpenAI message format. Handles tool calls in assistant messages by converting them
   * to ChatCompletionsFunctionToolCall objects.
   *
   * @param conversation llm4s conversation to convert
   * @return ArrayList of ChatRequestMessage suitable for OpenAI API
   */
  // TODO: Refactor to use idiomatic Scala collections with folding instead of mutable java.util.ArrayList
  private def convertToOpenAIMessages(conversation: Conversation): java.util.ArrayList[ChatRequestMessage] = {
    val messages = new java.util.ArrayList[ChatRequestMessage]()

    conversation.messages.foreach {
      case UserMessage(content) =>
        messages.add(new ChatRequestUserMessage(content))
      case SystemMessage(content) =>
        messages.add(new ChatRequestSystemMessage(content))
      case AssistantMessage(content, toolCalls) =>
        val msg = new ChatRequestAssistantMessage(content.getOrElse(""))
        // Add tool calls if needed
        if (toolCalls.nonEmpty) {
          val openAIToolCools = new java.util.ArrayList[ChatCompletionsToolCall]()
          toolCalls.foreach { tc =>
            val function = new FunctionCall(tc.name, tc.arguments.render())
            val toolCall = new ChatCompletionsFunctionToolCall(tc.id, function)
            openAIToolCools.add(toolCall)
          }
          msg.setToolCalls(openAIToolCools)
        }
        messages.add(msg)
      case ToolMessage(content, toolCallId) =>
        messages.add(new ChatRequestToolMessage(content, toolCallId))
    }

    messages
  }

  /**
   * Converts OpenAI ChatCompletions response to llm4s Completion format.
   *
   * Extracts the first choice from the response and converts it to llm4s format,
   * including content, tool calls, and token usage information.
   *
   * @param completions OpenAI API response
   * @return llm4s Completion with all response data
   */
  private def convertFromOpenAIFormat(completions: ChatCompletions): Completion = {
    val choice    = completions.getChoices.get(0)
    val message   = choice.getMessage
    val toolCalls = extractToolCalls(message)
    val content   = Option(message.getContent).getOrElse("")
    val assistantMessage =
      AssistantMessage(contentOpt = if (content.isEmpty) None else Some(content), toolCalls = toolCalls)

    Completion(
      id = completions.getId,
      created = completions.getCreatedAt.toEpochSecond,
      content = content,
      model = completions.getModel,
      message = assistantMessage,
      toolCalls = toolCalls.toList,
      usage = Option(completions.getUsage).map(u =>
        TokenUsage(
          promptTokens = u.getPromptTokens,
          completionTokens = u.getCompletionTokens,
          totalTokens = u.getTotalTokens
        )
      )
    )
  }

  /**
   * Extracts tool calls from an OpenAI response message.
   *
   * Parses function tool calls from the message and converts them to llm4s ToolCall format.
   * Returns empty sequence if no tool calls are present.
   *
   * @param message OpenAI response message potentially containing tool calls
   * @return sequence of ToolCall objects, empty if no tool calls present
   */
  private def extractToolCalls(message: ChatResponseMessage): Seq[ToolCall] =
    Option(message.getToolCalls)
      .map(_.asScala.toSeq.collect {
        case ftc: ChatCompletionsFunctionToolCall if Try(ujson.read(ftc.getFunction.getArguments)).isSuccess =>
          ToolCall(
            id = ftc.getId,
            name = ftc.getFunction.getName,
            arguments = Try(ujson.read(ftc.getFunction.getArguments)).getOrElse(ujson.Null)
          )
      })
      .getOrElse(Seq.empty)
}

/**
 * Factory methods for creating OpenAIClient instances.
 *
 * Provides safe construction of OpenAI clients with error handling via Result type.
 */
object OpenAIClient {
  import org.llm4s.types.TryOps

  /**
   * Creates an OpenAI client for direct OpenAI API access.
   *
   * @param config OpenAI configuration with API key, model, and base URL
   * @return Right(OpenAIClient) on success, Left(LLMError) if client creation fails
   */
  def apply(config: OpenAIConfig): Result[OpenAIClient] =
    Try(new OpenAIClient(config)).toResult

  /**
   * Creates an OpenAI client for Azure OpenAI service.
   *
   * @param config Azure configuration with API key, model, endpoint, and API version
   * @return Right(OpenAIClient) on success, Left(LLMError) if client creation fails
   */
  def apply(config: AzureConfig): Result[OpenAIClient] =
    Try(new OpenAIClient(config)).toResult
}
