package org.llm4s.toolapi

import org.llm4s.core.safety.Safety

import scala.concurrent.{ ExecutionContext, Future, blocking }
import java.util.concurrent.atomic.AtomicInteger
import scala.util.control.NonFatal

/**
 * Request model for tool calls
 */
case class ToolCallRequest(
  functionName: String,
  arguments: ujson.Value
)

/**
 * Registry for tool functions with execution capabilities.
 *
 * Supports both synchronous and asynchronous tool execution:
 * - `execute()` - Synchronous, blocking execution (original API)
 * - `executeAsync()` - Asynchronous, non-blocking execution
 * - `executeAll()` - Batch execution with configurable strategy
 */
class ToolRegistry(initialTools: Seq[ToolFunction[_, _]]) {

  def tools: Seq[ToolFunction[_, _]] = initialTools

  /**
   * Get a specific tool by name
   */
  def getTool(name: String): Option[ToolFunction[_, _]] = tools.find(_.name == name)

  /**
   * Execute a tool call synchronously
   */
  def execute(request: ToolCallRequest): Either[ToolCallError, ujson.Value] =
    tools.find(_.name == request.functionName) match {
      case Some(tool) =>
        Safety
          .safely(tool.execute(request.arguments))
          .left
          .map(err => ToolCallError.ExecutionError(request.functionName, new Exception(err.message)))
          .flatten
      case None => Left(ToolCallError.UnknownFunction(request.functionName))
    }

  /**
   * Execute a tool call asynchronously.
   *
   * Wraps synchronous execution in a Future for non-blocking operation.
   * NOTE: Tool execution typically involves blocking I/O.
   * We use `blocking` to hint the ExecutionContext to expand its pool if necessary.
   *
   * @param request The tool call request
   * @param ec ExecutionContext for async execution
   * @return Future containing the result
   */
  def executeAsync(request: ToolCallRequest)(implicit
    ec: ExecutionContext
  ): Future[Either[ToolCallError, ujson.Value]] =
    Future(blocking(execute(request)))

  /**
   * Execute multiple tool calls with a configurable strategy.
   *
   * @param requests The tool call requests to execute
   * @param strategy Execution strategy (Sequential, Parallel, or ParallelWithLimit)
   * @param ec ExecutionContext for async execution
   * @return Future containing results in the same order as requests
   */
  def executeAll(
    requests: Seq[ToolCallRequest],
    strategy: ToolExecutionStrategy = ToolExecutionStrategy.default
  )(implicit ec: ExecutionContext): Future[Seq[Either[ToolCallError, ujson.Value]]] =
    strategy match {
      case ToolExecutionStrategy.Sequential =>
        executeSequential(requests)

      case ToolExecutionStrategy.Parallel =>
        executeParallel(requests)

      case ToolExecutionStrategy.ParallelWithLimit(maxConcurrency) =>
        executeWithLimit(requests, maxConcurrency)
    }

  /**
   * Execute requests sequentially (one at a time).
   */
  private def executeSequential(
    requests: Seq[ToolCallRequest]
  )(implicit ec: ExecutionContext): Future[Seq[Either[ToolCallError, ujson.Value]]] =
    requests.foldLeft(Future.successful(Seq.empty[Either[ToolCallError, ujson.Value]])) { (accFuture, request) =>
      accFuture.flatMap(acc => executeAsync(request).map(result => acc :+ result))
    }

  /**
   * Execute all requests in parallel.
   */
  private def executeParallel(
    requests: Seq[ToolCallRequest]
  )(implicit ec: ExecutionContext): Future[Seq[Either[ToolCallError, ujson.Value]]] =
    Future.traverse(requests)(executeAsync)

  /**
   * Execute requests in parallel with a concurrency limit using a sliding window.
   * This implementation avoids Head-of-Line (HoL) blocking.
   */
  private def executeWithLimit(
    requests: Seq[ToolCallRequest],
    maxConcurrency: Int
  )(implicit ec: ExecutionContext): Future[Seq[Either[ToolCallError, ujson.Value]]] =
    if (requests.isEmpty) {
      Future.successful(Seq.empty)
    } else {
      val tasks        = requests.toVector
      val totalTasks   = tasks.length
      val currentIndex = new AtomicInteger(0)
      val results      = new Array[Either[ToolCallError, ujson.Value]](totalTasks)

      def worker(): Future[Unit] = {
        val idx = currentIndex.getAndIncrement()
        if (idx >= totalTasks) {
          Future.successful(())
        } else {
          val request = tasks(idx)
          executeAsync(request)
            .recover { case NonFatal(ex) =>
              Left(ToolCallError.ExecutionError(request.functionName, new Exception(ex.getMessage)))
            }
            .flatMap { result =>
              results(idx) = result
              worker()
            }
        }
      }

      val workerCount = math.min(maxConcurrency, totalTasks)
      val workers     = (1 to workerCount).map(_ => worker())

      Future.sequence(workers).map(_ => results.toSeq)
    }

  /**
   * Generate OpenAI tool definitions for all tools
   */
  def getOpenAITools(strict: Boolean = true): ujson.Arr =
    ujson.Arr.from(tools.map(_.toOpenAITool(strict)))

  /**
   * Generate a specific format of tool definitions for a particular LLM provider
   */
  def getToolDefinitions(provider: String): ujson.Value = provider.toLowerCase match {
    case "openai"    => getOpenAITools()
    case "anthropic" => getOpenAITools()
    case "gemini"    => getOpenAITools()
    case _           => throw new IllegalArgumentException(s"Unsupported LLM provider: $provider")
  }

  /**
   * Adds the tools from this registry to an Azure OpenAI ChatCompletionsOptions
   *
   * @param chatOptions The chat options to add the tools to
   * @return The updated chat options
   */
  def addToAzureOptions(
    chatOptions: com.azure.ai.openai.models.ChatCompletionsOptions
  ): com.azure.ai.openai.models.ChatCompletionsOptions =
    AzureToolHelper.addToolsToOptions(this, chatOptions)
}

object ToolRegistry {

  /**
   * Creates an empty tool registry with no tools
   */
  def empty: ToolRegistry = new ToolRegistry(Seq.empty)
}
