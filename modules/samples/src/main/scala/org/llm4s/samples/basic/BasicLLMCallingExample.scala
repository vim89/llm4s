package org.llm4s.samples.basic

import org.llm4s.llmconnect.LLMConnect
import org.llm4s.llmconnect.model._
import org.llm4s.samples.util.ConfigValidator

/**
 * Basic example demonstrating simple LLM API calls using LLM4S.
 *
 * This example shows:
 * - Setting up environment variables for different providers
 * - Creating a multi-turn conversation with system, user, and assistant messages
 * - Making a completion request and handling the response
 * - Displaying token usage information
 *
 * == Quick Start ==
 *
 * 1. Set your provider and model:
 *    {{{
 *    export LLM_MODEL=openai/gpt-4o
 *    }}}
 *
 * 2. Set your API key:
 *    {{{
 *    export OPENAI_API_KEY=sk-...
 *    }}}
 *
 * 3. Run the example:
 *    {{{
 *    sbt "samples/runMain org.llm4s.samples.basic.BasicLLMCallingExample"
 *    }}}
 *
 * == Expected Output ==
 * The LLM will respond with a function to filter even numbers from a list,
 * likely using the `isEven` function from the conversation history.
 * This demonstrates how conversation context helps the LLM provide coherent,
 * contextually relevant responses across multiple turns.
 *
 * == Supported Providers ==
 * - '''OpenAI''': `LLM_MODEL=openai/gpt-4o`, requires `OPENAI_API_KEY`
 * - '''Anthropic''': `LLM_MODEL=anthropic/claude-3-5-sonnet-latest`, requires `ANTHROPIC_API_KEY`
 * - '''Azure''': `LLM_MODEL=azure/<deployment>`, requires `AZURE_API_KEY` and `AZURE_API_BASE`
 * - '''Ollama''': `LLM_MODEL=ollama/llama2`, no API key needed (local)
 *
 * == Troubleshooting ==
 * If you see configuration errors, this example will guide you through
 * setting the correct environment variables for your chosen provider.
 *
 * For more information, see: https://github.com/llm4s/llm4s#getting-started
 */
object BasicLLMCallingExample {

  def main(args: Array[String]): Unit = {
    // Create a multi-turn conversation demonstrating different message types
    val conversation = Conversation(
      Seq(
        // System message: Defines the assistant's role and behavior
        // Sets the context for how the assistant should respond
        SystemMessage("You are a helpful programming assistant."),

        // User message: Initial request from the user
        UserMessage("Write a Scala function that checks if a number is even."),

        // Assistant message: Previous response in the conversation
        // Including conversation history helps maintain context across multiple turns
        AssistantMessage(
          """Here's a simple function to check if a number is even:
            |
            |```scala
            |def isEven(n: Int): Boolean = n % 2 == 0
            |```
            |
            |This uses the modulo operator (%) to check if the number is divisible by 2.""".stripMargin
        ),

        // User message: Follow-up question
        // The LLM can reference the previous conversation to provide a relevant answer
        UserMessage("Now write a function that filters a list to keep only even numbers.")
      )
    )

    // Execute the example with validation and error handling
    val result = for {
      // Validate configuration before attempting to connect
      _ <- ConfigValidator.validateEnvironment()
      // Get LLM client from environment variables
      client <- LLMConnect.fromEnv()
      // Make the completion request
      completion <- client.complete(conversation)
      _ = {
        // Display the response
        println(s"\nâœ… Success! Response from ${completion.model}")
        println(s"Model ID: ${completion.id}")
        println(s"Created at: ${completion.created}")
        println(s"Chat Role: ${completion.message.role}")
        println("\n--- Response ---")
        println(completion.message.content)
        println("--- End Response ---\n")

        // Print usage information if available
        completion.usage.foreach { usage =>
          println(
            s"ðŸ“Š Tokens used: ${usage.totalTokens} (${usage.promptTokens} prompt + ${usage.completionTokens} completion)"
          )
        }
      }
    } yield ()

    // Handle errors with helpful guidance
    result.fold(
      err => {
        println(err.formatted)
        println("\nðŸ’¡ Tip: Make sure your environment variables or application.conf values are set correctly.")
        println("For more help, see: https://github.com/llm4s/llm4s#getting-started")
        sys.exit(1)
      },
      identity
    )
  }
}
