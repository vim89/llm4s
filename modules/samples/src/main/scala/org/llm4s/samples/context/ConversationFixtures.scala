package org.llm4s.samples.context

import org.llm4s.llmconnect.model._

/**
 * Shared conversation fixtures for context management examples.
 * These conversations are designed to be large and realistic to properly
 * demonstrate compression and context management techniques.
 */
object ConversationFixtures {

  /**
   * Large realistic conversation (~16K+ tokens) with comprehensive technical content.
   * Perfect for demonstrating all context management pipeline steps.
   */
  val largeRealistic: Conversation = Conversation(
    Seq(
      UserMessage(
        "I'm setting up a distributed microservices architecture for an e-commerce platform. Can you help me design the system architecture, including service boundaries, data consistency patterns, communication protocols, monitoring strategies, and deployment approaches? I need to handle high traffic loads and ensure fault tolerance."
      ),
      AssistantMessage(
        "I'll help you design a comprehensive distributed microservices architecture for your e-commerce platform. Let me break this down into key areas: service boundaries, data management, communication, monitoring, and deployment strategies.\n\n**Service Boundaries & Domain Design**\n\nFor e-commerce, I recommend these core microservices based on business domains:\n\n1. **User Service**: Authentication, user profiles, preferences\n2. **Product Catalog Service**: Product information, search, categories\n3. **Inventory Service**: Stock levels, reservations, warehouse management\n4. **Order Service**: Order creation, order state management\n5. **Payment Service**: Payment processing, refunds, billing\n6. **Shipping Service**: Logistics, tracking, delivery management\n7. **Notification Service**: Email, SMS, push notifications\n8. **Review Service**: Product reviews, ratings\n9. **Recommendation Service**: ML-powered product suggestions\n10. **Analytics Service**: Business intelligence, reporting\n\nEach service should own its data and expose well-defined APIs. Use Domain-Driven Design (DDD) principles to identify bounded contexts and ensure services are cohesive with minimal coupling.\n\n**Data Consistency Patterns**\n\nFor distributed data management, you'll need different consistency patterns:\n\n- **Strong Consistency**: Critical for payments, inventory reservations\n- **Eventual Consistency**: Suitable for user preferences, analytics\n- **Saga Pattern**: For distributed transactions across services\n  - Order creation saga: Reserve inventory → Process payment → Create shipping → Send notifications\n- **Event Sourcing**: For audit trails and complex business logic\n- **CQRS**: Separate read/write models for performance optimization\n\n**Communication Protocols**\n\nImplement hybrid communication patterns:\n\n- **Synchronous**: REST/gRPC for real-time queries, user-facing operations\n- **Asynchronous**: Message queues (Apache Kafka, RabbitMQ) for event-driven workflows\n- **API Gateway**: Single entry point, routing, authentication, rate limiting\n- **Service Mesh**: (Istio, Linkerd) for service-to-service communication, security, observability"
      ),
      UserMessage(
        "This is great! Can you elaborate on the monitoring and observability strategy? What specific tools and patterns should I implement to ensure I can detect issues early and troubleshoot problems effectively in this distributed system?"
      ),
      AssistantMessage(
        "Excellent question! Observability is crucial for distributed systems. Let me outline a comprehensive monitoring strategy using the three pillars of observability: metrics, logs, and traces.\n\n**Metrics & Monitoring Stack**\n\n1. **Prometheus + Grafana**: \n   - Collect application metrics (request rate, latency, error rate)\n   - Infrastructure metrics (CPU, memory, disk, network)\n   - Business metrics (orders/second, revenue, conversion rates)\n   - Custom dashboards for each service and business domain\n\n2. **Key Metrics to Track**:\n   - **Golden Signals**: Latency, Traffic, Errors, Saturation\n   - **RED Method**: Rate, Errors, Duration\n   - **USE Method**: Utilization, Saturation, Errors\n   - **SLIs/SLOs**: Service Level Indicators and Objectives\n\n3. **Alerting Strategy**:\n   - **Multi-tier alerts**: Page-worthy, ticket-worthy, FYI\n   - **Alert fatigue prevention**: Proper thresholds, alert grouping\n   - **Escalation policies**: PagerDuty, OpsGenie integration\n\n**Distributed Tracing**\n\nImplement distributed tracing to understand request flows across services:\n\n- **Jaeger or Zipkin**: Trace requests across service boundaries\n- **OpenTelemetry**: Standardized instrumentation across all services\n- **Trace sampling**: Balance between visibility and performance overhead\n- **Service dependency mapping**: Understand service interactions and bottlenecks\n\n**Centralized Logging**\n\n1. **ELK Stack** (Elasticsearch, Logstash, Kibana):\n   - Structured logging with consistent format across services\n   - Log aggregation from all containers and services\n   - Correlation IDs to link logs across service calls\n\n2. **Log Strategy**:\n   - **Structured JSON logs**: Consistent parsing and querying\n   - **Log levels**: DEBUG, INFO, WARN, ERROR with proper usage\n   - **Security**: Avoid logging sensitive data (PII, passwords)\n   - **Retention policies**: Balance storage costs with compliance needs\n\n**Application Performance Monitoring (APM)**\n\n- **New Relic, Datadog, or Dynatrace**: End-to-end application monitoring\n- **Real User Monitoring (RUM)**: Frontend performance impact\n- **Synthetic monitoring**: Proactive health checks and user journey testing\n\n**Health Checks & Circuit Breakers**\n\n- **Health endpoints**: /health, /ready, /live for Kubernetes probes\n- **Circuit breaker pattern**: Prevent cascading failures (Netflix Hystrix, resilience4j)\n- **Bulkhead pattern**: Resource isolation\n- **Timeout and retry policies**: Fail fast, recover gracefully"
      ),
      UserMessage(
        "Perfect! Now let's talk about the deployment strategy. I'm thinking about using Kubernetes, but I want to understand the best practices for CI/CD pipelines, blue-green deployments, and how to handle database migrations in this microservices setup."
      ),
      ToolMessage(
        "deployment_analyzer",
        "{\"deployment_recommendations\": {\"container_orchestration\": \"kubernetes\", \"cluster_setup\": {\"production_clusters\": 3, \"regions\": [\"us-east-1\", \"us-west-2\", \"eu-west-1\"], \"node_types\": [\"spot_instances\", \"on_demand_fallback\"], \"auto_scaling\": {\"horizontal_pod_autoscaler\": true, \"vertical_pod_autoscaler\": true, \"cluster_autoscaler\": true}}, \"ci_cd_pipeline\": {\"stages\": [\"code_commit\", \"unit_tests\", \"integration_tests\", \"security_scan\", \"container_build\", \"registry_push\", \"staging_deploy\", \"e2e_tests\", \"production_deploy\"], \"tools\": [\"jenkins\", \"gitlab_ci\", \"github_actions\"], \"artifact_storage\": \"docker_registry\", \"security_scanning\": [\"sonarqube\", \"snyk\", \"twistlock\"]}, \"deployment_strategies\": {\"blue_green\": {\"benefits\": [\"zero_downtime\", \"instant_rollback\", \"full_testing\"], \"considerations\": [\"double_resources\", \"database_compatibility\"]}, \"canary\": {\"benefits\": [\"gradual_rollout\", \"risk_mitigation\", \"performance_monitoring\"], \"traffic_split\": \"5%_initial_25%_50%_100%\"}, \"rolling_updates\": {\"benefits\": [\"resource_efficient\", \"kubernetes_native\"], \"max_unavailable\": \"25%\", \"max_surge\": \"25%\"}}, \"database_migration\": {\"strategies\": [\"expand_contract\", \"parallel_run\", \"branch_by_abstraction\"], \"tools\": [\"flyway\", \"liquibase\", \"atlas\"], \"rollback_plan\": \"required\", \"backward_compatibility\": \"mandatory\"}, \"infrastructure_as_code\": {\"terraform\": \"cluster_provisioning\", \"helm\": \"application_deployment\", \"gitops\": \"argocd_flux\"}}"
      ),
      AssistantMessage(
        "Excellent choice with Kubernetes! Based on the deployment analysis, let me provide you with a comprehensive deployment strategy that addresses your CI/CD, blue-green deployments, and database migration concerns.\n\n**Kubernetes Deployment Architecture**\n\n1. **Multi-Region Setup**:\n   - Production clusters in 3 regions (us-east-1, us-west-2, eu-west-1)\n   - Mix of spot instances (cost optimization) with on-demand fallback\n   - Auto-scaling at multiple levels: HPA, VPA, and Cluster Autoscaler\n\n2. **Namespace Strategy**:\n   ```yaml\n   # Namespace organization\n   - production-ecommerce\n   - staging-ecommerce  \n   - development-ecommerce\n   - monitoring\n   - ingress-nginx\n   ```\n\n**CI/CD Pipeline Best Practices**\n\n1. **Pipeline Stages**:\n   ```\n   Code Commit → Unit Tests → Integration Tests → Security Scan → \n   Container Build → Registry Push → Staging Deploy → E2E Tests → \n   Production Deploy\n   ```\n\n2. **Implementation with GitOps**:\n   - **Source code**: GitHub/GitLab with feature branches\n   - **CI Pipeline**: GitHub Actions/GitLab CI for build and test\n   - **CD Pipeline**: ArgoCD for GitOps deployment\n   - **Infrastructure**: Terraform for cluster provisioning\n   - **Applications**: Helm charts for application deployment\n\n3. **Security Integration**:\n   - **SAST**: SonarQube for code quality and security\n   - **Container scanning**: Snyk/Twistlock for vulnerability detection\n   - **RBAC**: Kubernetes role-based access control\n   - **Secrets management**: HashiCorp Vault or AWS Secrets Manager\n\n**Deployment Strategies**\n\n1. **Blue-Green Deployment**:\n   ```yaml\n   # Suitable for: Critical services, major releases\n   apiVersion: argoproj.io/v1alpha1\n   kind: Rollout\n   metadata:\n     name: order-service\n   spec:\n     strategy:\n       blueGreen:\n         activeService: order-service-active\n         previewService: order-service-preview\n         autoPromotionEnabled: false\n         scaleDownDelaySeconds: 30\n         prePromotionAnalysis:\n           templates:\n           - templateName: health-check\n   ```\n\n2. **Canary Deployments**:\n   ```yaml\n   # Suitable for: User-facing services\n   strategy:\n     canary:\n       steps:\n       - setWeight: 5\n       - pause: {duration: 10m}\n       - setWeight: 25  \n       - pause: {duration: 30m}\n       - setWeight: 50\n       - pause: {duration: 1h}\n       - setWeight: 100\n   ```\n\n**Database Migration Strategy**\n\nThis is critical for microservices. Use the **Expand-Contract pattern**:\n\n1. **Phase 1 - Expand**:\n   ```sql\n   -- Add new column (backward compatible)\n   ALTER TABLE users ADD COLUMN email_verified BOOLEAN DEFAULT false;\n   -- Deploy application code that writes to both old and new fields\n   ```\n\n2. **Phase 2 - Migrate**:\n   ```sql\n   -- Migrate existing data\n   UPDATE users SET email_verified = (status = 'active') WHERE email_verified IS NULL;\n   ```\n\n3. **Phase 3 - Contract**:\n   ```sql\n   -- Remove old column after all services updated\n   ALTER TABLE users DROP COLUMN status;\n   ```\n\n**Database Migration Tools & Practices**:\n\n- **Flyway/Liquibase**: Version-controlled database migrations\n- **Init containers**: Run migrations before application startup\n- **Migration testing**: Always test migrations on production-like data\n- **Rollback strategy**: Every migration needs a rollback script\n- **Zero-downtime migrations**: Use online schema changes when possible"
      ),
      UserMessage(
        "This is incredibly comprehensive! One last area I'd like to explore: how do I handle configuration management across all these services? And what's the best approach for managing secrets, feature flags, and environment-specific configurations in Kubernetes?"
      ),
      ToolMessage(
        "config_analyzer",
        "{\"configuration_management\": {\"tools\": [\"kubernetes_configmaps\", \"kubernetes_secrets\", \"helm_values\", \"external_config_services\"], \"patterns\": {\"12_factor_app\": {\"config_in_environment\": true, \"no_secrets_in_code\": true, \"environment_parity\": true}, \"config_hierarchy\": [\"default_values\", \"environment_overrides\", \"service_specific\", \"runtime_injection\"]}, \"secrets_management\": {\"kubernetes_secrets\": {\"base64_encoded\": true, \"etcd_encrypted\": \"recommended\", \"limitations\": \"not_encrypted_at_rest_by_default\"}, \"external_solutions\": [\"hashicorp_vault\", \"aws_secrets_manager\", \"azure_key_vault\", \"google_secret_manager\"], \"best_practices\": [\"rotation_policy\", \"least_privilege_access\", \"audit_logging\", \"encryption_in_transit_and_rest\"]}, \"feature_flags\": {\"tools\": [\"launchdarkly\", \"unleash\", \"flagsmith\", \"split\"], \"patterns\": [\"percentage_rollout\", \"user_targeting\", \"kill_switches\", \"a_b_testing\"], \"integration\": \"sdk_or_api_based\"}, \"environment_management\": {\"environments\": [\"development\", \"staging\", \"production\"], \"config_sources\": [\"helm_values_files\", \"kubernetes_configmaps\", \"external_config_services\"], \"validation\": [\"schema_validation\", \"config_testing\", \"environment_promotion\"]}}"
      ),
      AssistantMessage(
        "Perfect! Configuration management is indeed crucial for microservices success. Let me break down a comprehensive approach for handling configuration, secrets, and feature flags in your Kubernetes environment.\n\n**Configuration Management Strategy**\n\n1. **Hierarchical Configuration Pattern**:\n   ```\n   Default Values (in code) → \n   Environment Config (Helm values) → \n   Service-Specific Config (ConfigMaps) → \n   Runtime Injection (External config services)\n   ```\n\n2. **Kubernetes Native Approach**:\n   ```yaml\n   # ConfigMap for non-sensitive config\n   apiVersion: v1\n   kind: ConfigMap\n   metadata:\n     name: order-service-config\n   data:\n     database.host: \"postgres.db.svc.cluster.local\"\n     database.port: \"5432\"\n     redis.ttl: \"3600\"\n     logging.level: \"INFO\"\n     payment.timeout: \"30s\"\n   ```\n\n3. **Helm Values Strategy**:\n   ```yaml\n   # values-production.yaml\n   orderService:\n     replicas: 3\n     resources:\n       requests:\n         cpu: 200m\n         memory: 512Mi\n       limits:\n         cpu: 500m\n         memory: 1Gi\n     config:\n       database:\n         maxConnections: 20\n         timeout: 30s\n       cache:\n         enabled: true\n         ttl: 3600\n   ```\n\n**Secrets Management Best Practices**\n\n1. **External Secrets Operator** (Recommended):\n   ```yaml\n   # Using HashiCorp Vault\n   apiVersion: external-secrets.io/v1beta1\n   kind: SecretStore\n   metadata:\n     name: vault-backend\n   spec:\n     provider:\n       vault:\n         server: \"https://vault.company.com\"\n         path: \"secret\"\n         version: \"v2\"\n   ---\n   apiVersion: external-secrets.io/v1beta1\n   kind: ExternalSecret\n   metadata:\n     name: database-credentials\n   spec:\n     secretStoreRef:\n       name: vault-backend\n       kind: SecretStore\n     target:\n       name: db-secret\n     data:\n     - secretKey: username\n       remoteRef:\n         key: database/prod\n         property: username\n     - secretKey: password\n       remoteRef:\n         key: database/prod\n         property: password\n   ```\n\n2. **Sealed Secrets** (Alternative approach):\n   ```bash\n   # Encrypt secrets for Git storage\n   echo -n mypassword | kubectl create secret generic mysecret --dry-run=client --from-file=password=/dev/stdin -o yaml | kubeseal -o yaml > mysealedsecret.yaml\n   ```\n\n**Feature Flags Implementation**\n\n1. **LaunchDarkly Integration** (Enterprise):\n   ```java\n   // Service code example\n   @Service\n   public class OrderService {\n       @Autowired\n       private LDClient ldClient;\n       \n       public void processOrder(Order order, User user) {\n           boolean useNewPaymentFlow = ldClient.boolVariation(\n               \"new-payment-flow\", user, false);\n           \n           if (useNewPaymentFlow) {\n               newPaymentProcessor.process(order);\n           } else {\n               legacyPaymentProcessor.process(order);\n           }\n       }\n   }\n   ```\n\n2. **Self-Hosted Unleash**:\n   ```yaml\n   # Unleash deployment\n   apiVersion: apps/v1\n   kind: Deployment\n   metadata:\n     name: unleash\n   spec:\n     template:\n       spec:\n         containers:\n         - name: unleash\n           image: unleashorg/unleash-server:latest\n           env:\n           - name: DATABASE_URL\n             valueFrom:\n               secretKeyRef:\n                 name: unleash-db\n                 key: url\n   ```\n\n**Environment-Specific Configuration**\n\n1. **Environment Promotion Strategy**:\n   ```\n   Development → Staging → Production\n   \n   Each environment has:\n   - Separate Helm values files\n   - Different ConfigMaps\n   - Environment-specific secrets\n   - Isolated namespaces\n   ```\n\n2. **Configuration Validation**:\n   ```yaml\n   # JSON Schema for config validation\n   configSchema:\n     type: object\n     required: [\"database\", \"redis\", \"payment\"]\n     properties:\n       database:\n         type: object\n         required: [\"host\", \"port\"]\n         properties:\n           host:\n             type: string\n           port:\n             type: integer\n             minimum: 1\n             maximum: 65535\n   ```\n\n**Configuration Best Practices**\n\n1. **12-Factor App Principles**:\n   - Store config in environment variables\n   - Never commit secrets to version control\n   - Maintain strict parity between environments\n   - Treat configuration as code\n\n2. **Configuration Testing**:\n   ```bash\n   # Helm template testing\n   helm template . --values values-production.yaml | kubeval\n   \n   # Config drift detection\n   kubectl diff -f k8s-manifests/\n   ```\n\n3. **Runtime Configuration Updates**:\n   ```yaml\n   # Hot reload support\n   apiVersion: v1\n   kind: Pod\n   spec:\n     containers:\n     - name: app\n       image: myapp:latest\n       volumeMounts:\n       - name: config\n         mountPath: /etc/config\n     volumes:\n     - name: config\n       configMap:\n         name: app-config\n   ```\n\n**Monitoring Configuration Changes**\n\n- **Config drift alerts**: Monitor for unauthorized changes\n- **Audit logging**: Track who changed what configuration\n- **Configuration validation**: Automated testing of config changes\n- **Rollback procedures**: Quick recovery from bad configuration\n\nThis approach gives you a robust, scalable configuration management system that grows with your microservices architecture while maintaining security and operational excellence."
      ),
      UserMessage(
        "Excellent! I have one more technical challenge. How do I implement effective caching strategies across these microservices? I'm particularly interested in distributed caching, cache invalidation patterns, and how to handle cache consistency in a distributed environment."
      ),
      ToolMessage(
        "cache_analyzer",
        "{\"caching_strategies\": {\"levels\": [\"browser_cache\", \"cdn_cache\", \"api_gateway_cache\", \"application_cache\", \"database_cache\"], \"distributed_caching\": {\"technologies\": [\"redis_cluster\", \"memcached\", \"hazelcast\", \"apache_ignite\"], \"patterns\": [\"cache_aside\", \"write_through\", \"write_behind\", \"refresh_ahead\"], \"consistency_models\": [\"strong_consistency\", \"eventual_consistency\", \"weak_consistency\"]}, \"invalidation_strategies\": {\"time_based\": {\"ttl\": \"time_to_live\", \"sliding_expiration\": \"reset_on_access\"}, \"event_based\": {\"cache_tags\": \"group_related_entries\", \"event_driven\": \"publish_subscribe_model\"}, \"version_based\": {\"versioned_keys\": \"key_versioning\", \"cache_stamps\": \"timestamp_validation\"}}, \"cache_patterns\": {\"cache_aside\": {\"description\": \"application_manages_cache\", \"use_case\": \"read_heavy_workloads\"}, \"write_through\": {\"description\": \"write_to_cache_and_db\", \"use_case\": \"data_consistency_critical\"}, \"write_behind\": {\"description\": \"async_db_write\", \"use_case\": \"write_heavy_workloads\"}, \"read_through\": {\"description\": \"cache_populates_itself\", \"use_case\": \"cache_miss_handling\"}}, \"data_structures\": {\"key_value\": \"simple_caching\", \"hash_maps\": \"structured_data\", \"sets\": \"unique_collections\", \"sorted_sets\": \"ranked_data\", \"streams\": \"event_logs\", \"pub_sub\": \"real_time_updates\"}}"
      ),
      AssistantMessage(
        "Excellent question! Caching is critical for microservices performance and scalability. Let me outline a comprehensive caching strategy that addresses distributed caching, invalidation patterns, and consistency challenges.\n\n**Multi-Level Caching Architecture**\n\n1. **Caching Hierarchy**:\n   ```\n   Browser Cache (Client-side)\n   ↓\n   CDN Cache (Edge locations)\n   ↓  \n   API Gateway Cache (Entry point)\n   ↓\n   Application Cache (Service-level)\n   ↓\n   Database Cache (Data layer)\n   ```\n\n2. **Distributed Redis Cluster Setup**:\n   ```yaml\n   # Redis Cluster for high availability\n   apiVersion: apps/v1\n   kind: StatefulSet\n   metadata:\n     name: redis-cluster\n   spec:\n     serviceName: redis-cluster\n     replicas: 6  # 3 masters + 3 replicas\n     template:\n       spec:\n         containers:\n         - name: redis\n           image: redis:7-alpine\n           ports:\n           - containerPort: 6379\n           - containerPort: 16379  # cluster bus port\n           command:\n           - redis-server\n           - /etc/redis/redis.conf\n           - --cluster-enabled yes\n           - --cluster-config-file nodes.conf\n           - --cluster-node-timeout 5000\n   ```\n\n**Caching Patterns Implementation**\n\n1. **Cache-Aside Pattern** (Most common):\n   ```java\n   @Service\n   public class ProductService {\n       @Autowired\n       private RedisTemplate<String, Product> redisTemplate;\n       \n       @Autowired\n       private ProductRepository productRepo;\n       \n       public Product getProduct(String productId) {\n           // Try cache first\n           String cacheKey = \"product:\" + productId;\n           Product product = redisTemplate.opsForValue().get(cacheKey);\n           \n           if (product == null) {\n               // Cache miss - fetch from database\n               product = productRepo.findById(productId);\n               if (product != null) {\n                   // Store in cache with TTL\n                   redisTemplate.opsForValue().set(cacheKey, product, \n                       Duration.ofMinutes(30));\n               }\n           }\n           return product;\n       }\n   }\n   ```\n\n2. **Write-Through Pattern** (For critical data):\n   ```java\n   public void updateProduct(Product product) {\n       // Write to database first\n       productRepo.save(product);\n       \n       // Then update cache\n       String cacheKey = \"product:\" + product.getId();\n       redisTemplate.opsForValue().set(cacheKey, product, \n           Duration.ofMinutes(30));\n   }\n   ```\n\n3. **Write-Behind Pattern** (For high-volume writes):\n   ```java\n   @Async\n   public void updateProductAsync(Product product) {\n       // Update cache immediately\n       String cacheKey = \"product:\" + product.getId();\n       redisTemplate.opsForValue().set(cacheKey, product);\n       \n       // Queue for async database update\n       productUpdateQueue.send(product);\n   }\n   ```\n\n**Cache Invalidation Strategies**\n\n1. **Time-Based Invalidation (TTL)**:\n   ```java\n   // Simple TTL\n   redisTemplate.opsForValue().set(\"key\", value, Duration.ofHours(1));\n   \n   // Sliding expiration\n   @Cacheable(value = \"products\", key = \"#productId\")\n   @CacheEvict(value = \"products\", key = \"#productId\", \n               condition = \"#result != null\")\n   public Product getProductWithSlidingExpiry(String productId) {\n       return productRepo.findById(productId);\n   }\n   ```\n\n2. **Event-Driven Invalidation**:\n   ```java\n   @EventListener\n   public void handleProductUpdated(ProductUpdatedEvent event) {\n       // Invalidate related cache entries\n       String productKey = \"product:\" + event.getProductId();\n       String categoryKey = \"category:\" + event.getCategoryId();\n       \n       redisTemplate.delete(Arrays.asList(productKey, categoryKey));\n       \n       // Publish cache invalidation event to other services\n       cacheInvalidationPublisher.publish(new CacheInvalidationEvent(\n           \"product\", event.getProductId()));\n   }\n   ```\n\n3. **Cache Tags for Group Invalidation**:\n   ```java\n   public class TaggedCache {\n       public void setWithTags(String key, Object value, Set<String> tags) {\n           // Store the value\n           redisTemplate.opsForValue().set(key, value);\n           \n           // Associate with tags\n           tags.forEach(tag -> {\n               String tagKey = \"tag:\" + tag;\n               redisTemplate.opsForSet().add(tagKey, key);\n           });\n       }\n       \n       public void invalidateByTag(String tag) {\n           String tagKey = \"tag:\" + tag;\n           Set<String> keys = redisTemplate.opsForSet().members(tagKey);\n           \n           if (!keys.isEmpty()) {\n               redisTemplate.delete(keys);\n               redisTemplate.delete(tagKey);\n           }\n       }\n   }\n   ```\n\n**Distributed Cache Consistency**\n\n1. **Eventual Consistency with Event Sourcing**:\n   ```java\n   @Service\n   public class CacheConsistencyService {\n       \n       @KafkaListener(topics = \"product.events\")\n       public void handleProductEvent(ProductEvent event) {\n           switch (event.getEventType()) {\n               case CREATED:\n               case UPDATED:\n                   // Warm up cache with latest data\n                   warmUpProductCache(event.getProductId());\n                   break;\n               case DELETED:\n                   // Remove from cache\n                   invalidateProductCache(event.getProductId());\n                   break;\n           }\n       }\n   }\n   ```\n\n2. **Version-Based Consistency**:\n   ```java\n   public class VersionedCache {\n       public void setVersioned(String key, Object value, long version) {\n           String versionedKey = key + \":v\" + version;\n           redisTemplate.opsForValue().set(versionedKey, value);\n           redisTemplate.opsForValue().set(key + \":latest\", version);\n       }\n       \n       public Object getVersioned(String key) {\n           Long latestVersion = redisTemplate.opsForValue().get(key + \":latest\");\n           if (latestVersion != null) {\n               return redisTemplate.opsForValue().get(key + \":v\" + latestVersion);\n           }\n           return null;\n       }\n   }\n   ```\n\n**Advanced Caching Strategies**\n\n1. **Circuit Breaker for Cache**:\n   ```java\n   @Component\n   public class CacheCircuitBreaker {\n       private final CircuitBreaker circuitBreaker;\n       \n       public <T> T getWithFallback(String key, Supplier<T> fallback) {\n           return circuitBreaker.executeSupplier(() -> {\n               T cached = redisTemplate.opsForValue().get(key);\n               return cached != null ? cached : fallback.get();\n           });\n       }\n   }\n   ```\n\n2. **Cache Warming Strategy**:\n   ```java\n   @Scheduled(cron = \"0 0 1 * * ?\")  // Daily at 1 AM\n   public void warmUpCache() {\n       // Pre-load frequently accessed data\n       List<String> popularProducts = analyticsService.getPopularProducts();\n       \n       popularProducts.parallelStream().forEach(productId -> {\n           Product product = productRepo.findById(productId);\n           if (product != null) {\n               String cacheKey = \"product:\" + productId;\n               redisTemplate.opsForValue().set(cacheKey, product, \n                   Duration.ofHours(12));\n           }\n       });\n   }\n   ```\n\n**Cache Monitoring & Observability**\n\n1. **Key Metrics to Track**:\n   - Cache hit/miss ratios per service\n   - Cache response times\n   - Memory usage and eviction rates\n   - Network latency to cache cluster\n\n2. **Redis Monitoring**:\n   ```yaml\n   # Prometheus Redis exporter\n   apiVersion: apps/v1\n   kind: Deployment\n   metadata:\n     name: redis-exporter\n   spec:\n     template:\n       spec:\n         containers:\n         - name: redis-exporter\n           image: oliver006/redis_exporter:latest\n           env:\n           - name: REDIS_ADDR\n             value: \"redis://redis-cluster:6379\"\n   ```\n\nThis comprehensive caching strategy provides high performance, consistency, and reliability across your distributed microservices architecture while maintaining flexibility for different use cases and data patterns."
      ),
      UserMessage(
        "This has been incredibly helpful! As a final question, what about disaster recovery and backup strategies for this entire distributed system? How do I ensure business continuity, handle regional failures, and implement effective backup and restore procedures across all these services and data stores?"
      ),
      ToolMessage(
        "disaster_recovery_analyzer",
        "{\"disaster_recovery_strategies\": {\"rto_rpo_targets\": {\"tier_1_critical\": {\"rto\": \"< 1 hour\", \"rpo\": \"< 5 minutes\"}, \"tier_2_important\": {\"rto\": \"< 4 hours\", \"rpo\": \"< 30 minutes\"}, \"tier_3_standard\": {\"rto\": \"< 24 hours\", \"rpo\": \"< 4 hours\"}}, \"multi_region_strategies\": {\"active_active\": {\"benefits\": [\"zero_downtime\", \"load_distribution\", \"instant_failover\"], \"complexity\": \"high\", \"cost\": \"high\"}, \"active_passive\": {\"benefits\": [\"cost_effective\", \"simpler_setup\", \"data_consistency\"], \"rto\": \"minutes_to_hours\"}, \"pilot_light\": {\"benefits\": [\"cost_optimized\", \"quick_scale_up\"], \"rto\": \"30_minutes_to_2_hours\"}}, \"backup_strategies\": {\"database_backups\": {\"postgresql\": [\"continuous_wal_archiving\", \"point_in_time_recovery\", \"logical_backups\"], \"mongodb\": [\"replica_set_snapshots\", \"oplog_backups\", \"sharded_cluster_backups\"], \"redis\": [\"rdb_snapshots\", \"aof_logs\", \"cluster_backups\"]}, \"application_backups\": [\"container_images\", \"configuration_backups\", \"secrets_backups\", \"infrastructure_as_code\"]}, \"failover_mechanisms\": {\"dns_failover\": \"route53_health_checks\", \"load_balancer_failover\": \"cross_region_lb\", \"kubernetes_failover\": \"multi_cluster_mesh\", \"database_failover\": \"automatic_replica_promotion\"}, \"testing_procedures\": [\"disaster_recovery_drills\", \"chaos_engineering\", \"failover_testing\", \"backup_restoration_testing\"]}}"
      ),
      AssistantMessage(
        "Excellent final question! Disaster recovery is absolutely critical for mission-critical e-commerce systems. Let me outline a comprehensive DR strategy that ensures business continuity across your entire distributed architecture.\n\n**Disaster Recovery Tiers & Objectives**\n\n1. **Service Classification**:\n   - **Tier 1 (Critical)**: Order processing, payments, authentication\n     - RTO: < 1 hour, RPO: < 5 minutes\n   - **Tier 2 (Important)**: Product catalog, inventory, notifications  \n     - RTO: < 4 hours, RPO: < 30 minutes\n   - **Tier 3 (Standard)**: Analytics, recommendations, reviews\n     - RTO: < 24 hours, RPO: < 4 hours\n\n**Multi-Region Architecture Strategy**\n\n1. **Active-Active Setup for Critical Services**:\n   ```yaml\n   # Global load balancer configuration\n   apiVersion: networking.istio.io/v1beta1\n   kind: Gateway\n   metadata:\n     name: global-gateway\n   spec:\n     selector:\n       istio: ingressgateway\n     servers:\n     - port:\n         number: 443\n         name: https\n         protocol: HTTPS\n       hosts:\n       - api.ecommerce.com\n       tls:\n         mode: SIMPLE\n         credentialName: tls-secret\n   ---\n   # Traffic splitting across regions\n   apiVersion: networking.istio.io/v1beta1\n   kind: VirtualService\n   metadata:\n     name: global-routing\n   spec:\n     hosts:\n     - api.ecommerce.com\n     http:\n     - match:\n       - headers:\n           region:\n             exact: \"us-east\"\n       route:\n       - destination:\n           host: api-gateway-us-east.ecommerce.svc.cluster.local\n         weight: 100\n     - match:\n       - headers:\n           region:\n             exact: \"us-west\"\n       route:\n       - destination:\n           host: api-gateway-us-west.ecommerce.svc.cluster.local\n         weight: 100\n     - route:  # Default routing with failover\n       - destination:\n           host: api-gateway-us-east.ecommerce.svc.cluster.local\n         weight: 70\n       - destination:\n           host: api-gateway-us-west.ecommerce.svc.cluster.local\n         weight: 30\n   ```\n\n2. **DNS-Based Failover with Health Checks**:\n   ```json\n   {\n     \"Type\": \"A\",\n     \"Name\": \"api.ecommerce.com\",\n     \"SetIdentifier\": \"primary\",\n     \"Failover\": \"PRIMARY\",\n     \"TTL\": 60,\n     \"ResourceRecords\": [\n       {\"Value\": \"52.1.1.1\"}\n     ],\n     \"HealthCheckId\": \"primary-health-check\"\n   },\n   {\n     \"Type\": \"A\", \n     \"Name\": \"api.ecommerce.com\",\n     \"SetIdentifier\": \"secondary\",\n     \"Failover\": \"SECONDARY\",\n     \"TTL\": 60,\n     \"ResourceRecords\": [\n       {\"Value\": \"54.2.2.2\"}\n     ]\n   }\n   ```\n\n**Database Disaster Recovery**\n\n1. **PostgreSQL Multi-Region Setup**:\n   ```yaml\n   # Primary database cluster\n   apiVersion: postgresql.cnpg.io/v1\n   kind: Cluster\n   metadata:\n     name: postgres-primary\n   spec:\n     instances: 3\n     primaryUpdateStrategy: unsupervised\n     \n     postgresql:\n       parameters:\n         max_connections: \"200\"\n         shared_preload_libraries: \"pg_stat_statements\"\n         wal_level: \"logical\"\n         archive_mode: \"on\"\n         archive_command: \"aws s3 cp %p s3://ecommerce-wal-backup/%f\"\n     \n     backup:\n       retentionPolicy: \"30d\"\n       barmanObjectStore:\n         destinationPath: \"s3://ecommerce-db-backup\"\n         s3Credentials:\n           accessKeyId:\n             name: backup-creds\n             key: ACCESS_KEY_ID\n           secretAccessKey:\n             name: backup-creds  \n             key: SECRET_ACCESS_KEY\n         wal:\n           retention: \"7d\"\n   ---\n   # Read replica in secondary region\n   apiVersion: postgresql.cnpg.io/v1\n   kind: Cluster\n   metadata:\n     name: postgres-replica-west\n   spec:\n     instances: 2\n     \n     replica:\n       enabled: true\n       source: \"postgres-primary\"\n   ```\n\n2. **Automated Database Backups**:\n   ```bash\n   #!/bin/bash\n   # Automated backup script\n   \n   # Full backup daily\n   pg_basebackup -h primary.db.local -D /backup/$(date +%Y%m%d) \\\n     -U replication -W -P -x\n   \n   # Upload to S3 with encryption\n   aws s3 sync /backup/$(date +%Y%m%d) \\\n     s3://ecommerce-db-backup/daily/$(date +%Y%m%d) \\\n     --sse AES256\n   \n   # Point-in-time recovery setup\n   archive_command = 'aws s3 cp %p s3://ecommerce-wal-backup/$(date +%Y%m%d)/%f'\n   ```\n\n**Application & Configuration Backups**\n\n1. **GitOps Backup Strategy**:\n   ```yaml\n   # Backup CronJob for configurations\n   apiVersion: batch/v1\n   kind: CronJob\n   metadata:\n     name: config-backup\n   spec:\n     schedule: \"0 2 * * *\"  # Daily at 2 AM\n     jobTemplate:\n       spec:\n         template:\n           spec:\n             containers:\n             - name: backup\n               image: bitnami/kubectl:latest\n               command:\n               - /bin/sh\n               - -c\n               - |\n                 # Backup all ConfigMaps\n                 kubectl get configmaps --all-namespaces -o yaml > /backup/configmaps.yaml\n                 \n                 # Backup all Secrets (encrypted)\n                 kubectl get secrets --all-namespaces -o yaml > /backup/secrets.yaml\n                 \n                 # Upload to S3\n                 aws s3 cp /backup/ s3://ecommerce-k8s-backup/$(date +%Y%m%d)/ --recursive\n             restartPolicy: OnFailure\n   ```\n\n2. **Container Registry Backup**:\n   ```bash\n   # Replicate container images across regions\n   aws ecr create-repository --repository-name ecommerce/order-service \\\n     --region us-west-2\n   \n   # Cross-region replication rule\n   aws ecr put-replication-configuration --replication-configuration \\\n     'rules=[{destinations=[{region=us-west-2,registryId=123456789012}]}]'\n   ```\n\n**Automated Failover Mechanisms**\n\n1. **Kubernetes Multi-Cluster Failover**:\n   ```yaml\n   # Admiral for multi-cluster service mesh\n   apiVersion: install.istio.io/v1alpha1\n   kind: IstioOperator\n   metadata:\n     name: admiral\n   spec:\n     values:\n       pilot:\n         env:\n           ADMIRAL_SYNC_NAMESPACE: admiral-sync\n           ADMIRAL_CACHE_REFRESH_DURATION: 5m\n   ```\n\n2. **Database Automatic Failover**:\n   ```yaml\n   # Patroni for PostgreSQL HA\n   apiVersion: v1\n   kind: ConfigMap\n   metadata:\n     name: patroni-config\n   data:\n     patroni.yml: |\n       scope: postgres-cluster\n       name: postgres-1\n       \n       restapi:\n         listen: 0.0.0.0:8008\n         connect_address: postgres-1:8008\n       \n       etcd3:\n         hosts: etcd-1:2379,etcd-2:2379,etcd-3:2379\n       \n       bootstrap:\n         dcs:\n           ttl: 30\n           loop_wait: 10\n           retry_timeout: 30\n           maximum_lag_on_failover: 1048576\n           \n       postgresql:\n         listen: 0.0.0.0:5432\n         connect_address: postgres-1:5432\n         data_dir: /var/lib/postgresql/data\n         parameters:\n           hot_standby: \"on\"\n           wal_level: hot_standby\n           max_wal_senders: 5\n           max_replication_slots: 5\n   ```\n\n**Disaster Recovery Testing & Validation**\n\n1. **Automated DR Drills**:\n   ```yaml\n   # Chaos engineering with Litmus\n   apiVersion: litmuschaos.io/v1alpha1\n   kind: ChaosExperiment\n   metadata:\n     name: region-failure-simulation\n   spec:\n     definition:\n       scope: Cluster\n       permissions:\n       - apiGroups: [\"\"]\n         resources: [\"pods\", \"services\"]\n         verbs: [\"create\", \"delete\", \"get\", \"list\"]\n       image: \"litmuschaos/ansible-runner:latest\"\n       args:\n       - -c\n       - ansible-playbook ./experiments/region-failure/ansible/experiment.yml\n       command:\n       - /bin/bash\n   ```\n\n2. **Recovery Time Testing**:\n   ```bash\n   #!/bin/bash\n   # DR testing script\n   \n   echo \"Starting DR test at $(date)\"\n   \n   # Simulate regional failure\n   kubectl patch deployment api-gateway-us-east -p '{\"spec\":{\"replicas\":0}}'\n   \n   # Measure failover time\n   start_time=$(date +%s)\n   while ! curl -s https://api.ecommerce.com/health > /dev/null; do\n     sleep 5\n   done\n   end_time=$(date +%s)\n   \n   failover_time=$((end_time - start_time))\n   echo \"Failover completed in $failover_time seconds\"\n   \n   # Validate all services are functional\n   ./validate-services.sh\n   \n   # Restore primary region\n   kubectl patch deployment api-gateway-us-east -p '{\"spec\":{\"replicas\":3}}'\n   ```\n\n**Business Continuity Procedures**\n\n1. **Incident Response Playbook**:\n   ```yaml\n   # PagerDuty escalation policy\n   escalation_policy:\n     name: \"Disaster Recovery\"\n     escalation_rules:\n     - escalation_delay_in_minutes: 0\n       targets:\n       - type: \"user\"\n         id: \"on-call-engineer\"\n     - escalation_delay_in_minutes: 15  \n       targets:\n       - type: \"user\"\n         id: \"senior-engineer\"\n     - escalation_delay_in_minutes: 30\n       targets:\n       - type: \"user\"\n         id: \"engineering-manager\"\n   ```\n\n2. **Communication Templates**:\n   ```markdown\n   # Incident Communication Template\n   \n   ## Service Disruption Alert\n   \n   **Status**: Investigating\n   **Impact**: [Tier 1/2/3] services affected\n   **ETA**: Recovery within [RTO target]\n   \n   **Current Actions**:\n   - Failover to secondary region initiated\n   - Database replica promotion in progress\n   - Traffic routing updated\n   \n   **Next Update**: In 15 minutes\n   ```\n\nThis comprehensive disaster recovery strategy ensures your e-commerce platform can withstand regional failures, data center outages, and various disaster scenarios while maintaining business continuity and meeting your RTO/RPO objectives."
      ),
      UserMessage(
        "This has been absolutely phenomenal! You've covered everything from architecture design to disaster recovery. I feel much more confident about building this distributed e-commerce system. Thank you for such detailed and practical guidance!"
      )
    )
  )

  /**
   * Tool-heavy conversation with large outputs for demonstrating tool compression.
   * Contains realistic system monitoring data, logs, and diagnostic information.
   */
  val toolHeavy: Conversation = Conversation(
    Seq(
      UserMessage(
        "I need help analyzing our production system performance. Can you check the current system metrics, recent error logs, and database performance indicators?"
      ),
      AssistantMessage(
        "I'll help you analyze your production system performance. Let me gather comprehensive metrics and logs to identify any issues or bottlenecks."
      ),
      ToolMessage(createLargeSystemMetrics(), "system_metrics"),
      AssistantMessage(
        "I can see some concerning patterns in the system metrics. CPU usage is spiking across multiple servers, and memory utilization is quite high. Let me check the recent error logs to correlate these issues."
      ),
      ToolMessage(createLargeErrorLogs(), "error_logs"),
      AssistantMessage(
        "The error logs reveal several critical issues: database connection timeouts, memory pressure warnings, and some failed requests. Let me now examine the database performance metrics to understand the root cause."
      ),
      ToolMessage(createLargeDatabaseMetrics(), "db_metrics"),
      AssistantMessage("I also need to examine some binary diagnostic data that was captured during the incident."),
      ToolMessage(createLargeBase64Blob(), "binary_dump"),
      AssistantMessage(
        "Based on the comprehensive analysis, I've identified several performance bottlenecks that need immediate attention. The system is experiencing high load with database connection pool exhaustion and memory pressure across multiple services."
      )
    )
  )

  /**
   * Conversation with repetitive content for testing deterministic compression.
   */
  val repetitive: Conversation = Conversation(
    Seq(
      UserMessage("Explain the key concepts of software engineering."),
      AssistantMessage(
        "The key concepts of software engineering include design patterns, testing methodologies, and code organization. For example, design patterns provide reusable solutions. For instance, the Observer pattern is widely used. For example, in MVC architectures. Actually, let me elaborate on this further. Basically, these concepts are fundamental. You know, understanding these principles is crucial. Actually, design patterns are really important. For instance, Singleton ensures single instance. For example, database connections often use this. As I mentioned, these patterns are essential. Like I said, they solve common problems. Actually, the key point here is reusability."
      ),
      UserMessage("Can you provide more examples?"),
      AssistantMessage(
        "Certainly! For example, the Factory pattern creates objects. For instance, when you need different types of objects. For example, creating different types of vehicles. Actually, this pattern is quite useful. Basically, it provides flexibility. You know, it's widely adopted. Actually, another example is the Strategy pattern. For instance, different sorting algorithms. For example, QuickSort vs MergeSort. As I mentioned before, patterns solve recurring problems. Like I said earlier, they're fundamental concepts."
      )
    )
  )

  /**
   * Mixed content conversation with various data types for comprehensive testing.
   */
  val mixed: Conversation = Conversation(
    Seq(
      UserMessage("Show me examples of different data formats we might encounter in a microservices system."),
      AssistantMessage(
        "Here are various data formats commonly used in microservices:\n\n**JSON Configuration:**\n```json\n{\n  \"database\": {\n    \"host\": \"postgres.service.consul\",\n    \"port\": 5432,\n    \"ssl\": true,\n    \"pool_size\": 20\n  },\n  \"redis\": {\n    \"cluster\": [\"redis-1:6379\", \"redis-2:6379\", \"redis-3:6379\"]\n  }\n}\n```"
      ),
      UserMessage("What about log formats and monitoring data?"),
      AssistantMessage(
        "Here's typical structured logging and monitoring data:\n\n**Application Logs:**\n```\n2024-01-15T10:30:45.123Z INFO [order-service] Processing order request order_id=12345 user_id=67890 amount=99.99\n2024-01-15T10:30:45.456Z WARN [payment-service] High response time detected payment_id=54321 duration=2.5s threshold=2.0s\n2024-01-15T10:30:45.789Z ERROR [inventory-service] Stock reservation failed product_id=98765 requested=5 available=3\n```\n\n**Prometheus Metrics:**\n```\nhttp_requests_total{method=\"GET\",status=\"200\",service=\"order\"} 12543\nhttp_request_duration_seconds{method=\"POST\",service=\"payment\",quantile=\"0.95\"} 0.125\ndatabase_connections_active{service=\"inventory\",db=\"postgres\"} 15\n```"
      ),
      UserMessage("And what about error traces and diagnostic information?"),
      AssistantMessage(
        "Here are typical error traces and diagnostic data:\n\n**Stack Traces:**\n```\njava.sql.SQLException: Connection timeout\n  at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:225)\n  at com.company.service.DatabaseService.getConnection(DatabaseService.java:45)\n  at com.company.service.OrderService.createOrder(OrderService.java:123)\n  at com.company.controller.OrderController.processOrder(OrderController.java:67)\n```\n\n**Kubernetes Events:**\n```yaml\nevents:\n- type: Warning\n  reason: FailedScheduling\n  message: \"0/3 nodes are available: 3 Insufficient memory\"\n- type: Normal\n  reason: Scheduled\n  message: \"Successfully assigned default/order-service-abc123 to node-2\"\n```"
      ),
      UserMessage("Can you also check the current system health?"),
      ToolMessage(createSmallMetrics(), "small_metrics"),
      AssistantMessage(
        "The system health looks good - all services are running normally with acceptable latency metrics."
      )
    )
  )

  /**
   * Small verbose conversation with filler words and repetitive patterns.
   * Perfect for testing deterministic compression rules on verbose content.
   */
  val smallVerbose: Conversation = Conversation(
    Seq(
      UserMessage("Can you explain the weather API response?"),
      AssistantMessage(
        "Well, you know, the weather API response is, like, really important. " +
          "Actually, it contains various fields. For example, temperature data. " +
          "For instance, current temp, min temp, max temp. As I mentioned, humidity is included. " +
          "Like I said before, wind speed matters too. Actually, the response format varies. " +
          "You know, JSON is common. Basically, you parse the fields you need."
      ),
      UserMessage("What about error handling?"),
      AssistantMessage(
        "Oh, error handling! Well, that's super important, you know. For example, network timeouts. " +
          "For instance, 404 errors when endpoints change. As I mentioned earlier, rate limiting. " +
          "Like I said, validation matters. Actually, you should check response status. " +
          "You know, 200 means success. Basically, handle exceptions gracefully. " +
          "For example, retry logic. For instance, exponential backoff."
      )
    )
  )

  // Helper methods to create large realistic content
  private def createLargeSystemMetrics(): String = {
    val servers = (1 to 60).map { i =>
      s"""\"server-$i\": {
         |    \"cpu_usage_percent\": ${(i * 7)     % 95 + 5},
         |    \"memory_usage_percent\": ${(i * 11) % 85 + 15},
         |    \"disk_usage_percent\": ${(i * 5)    % 75 + 25},
         |    \"network_in_mbps\": ${(i * 23)      % 500 + 50},
         |    \"network_out_mbps\": ${(i * 17)     % 400 + 30},
         |    \"load_average\": [${i               % 8 + 1}.${i % 10}, ${i % 8 + 2}.${(i + 1) % 10}, ${i % 8 + 3}.${(i + 2) % 10}],
         |    \"active_connections\": ${(i * 31)   % 1000 + 100},
         |    \"processes\": ${(i * 13)            % 300 + 50},
         |    \"open_files\": ${(i * 41)           % 5000 + 500}
         |  }""".stripMargin
    }

    s"""{
       |  "timestamp": "2024-01-15T15:30:00Z",
       |  "cluster": "production-cluster",
       |  "region": "us-east-1",
       |  "total_servers": 60,
       |  "server_metrics": {
       |${servers.mkString(",\n")}
       |  }
       |}""".stripMargin
  }

  private def createLargeErrorLogs(): String = {
    val logLines = (1 to 2000).map { i =>
      val hh    = f"${(i / 60) % 24}%02d"
      val mm    = f"${i % 60}%02d"
      val ss    = f"${(i * 7) % 60}%02d"
      val ms    = f"${(i * 11) % 1000}%03d"
      val level = if (i % 15 == 0) "ERROR" else if (i % 8 == 0) "WARN" else "INFO"
      val service =
        Seq("order-service", "payment-service", "inventory-service", "user-service", "notification-service")(i % 5)
      val reqId = f"req-${i}%06d"

      val msg = level match {
        case "ERROR" =>
          s"Database connection timeout after ${i % 30 + 10}s - connection_pool_exhausted retry_attempt=${i % 3 + 1}/3"
        case "WARN" =>
          s"High memory usage detected ${70 + (i % 25)}% of 8GB heap - gc_pressure_increasing consider_scaling"
        case _ =>
          s"Request processed successfully user_id=${i % 10000} request_id=$reqId duration=${i % 500 + 50}ms"
      }

      s"2024-01-15T$hh:$mm:$ss.${ms}Z $level [$service] [thread-${i % 20}] $msg"
    }

    "Production Error Log Analysis - Last 2 hours\n" +
      "Log entries: 2000 lines\n\n" +
      logLines.mkString("\n") + "\n"
  }

  private def createLargeDatabaseMetrics(): String = {
    val tables = (1 to 120).map { i =>
      s""""table_$i": {
         |    "row_count": ${i * 50000 + (i  % 1000) * 500},
         |    "size_mb": ${i * 125 + (i      % 50) * 10},
         |    "avg_query_time_ms": ${(i      % 300) + 25},
         |    "slow_queries_last_hour": ${i  % 15},
         |    "index_hit_ratio": ${0.75 + (i % 25) * 0.01},
         |    "table_scans": ${i             % 8},
         |    "index_scans": ${(i * 47)      % 1000 + 100}
         |  }""".stripMargin
    }

    s"""{
       |  "database_performance_snapshot": {
       |    "timestamp": "2024-01-15T15:35:00Z",
       |    "database_name": "ecommerce_prod",
       |    "table_statistics": {
       |${tables.mkString(",\n")}
       |    }
       |  }
       |}""".stripMargin
  }

  private def createLargeBase64Blob(): String = {
    val chunk = "QUJDREVGR0hJSktMTU5PUFFSU1RVVldYWVo=" // base64 for "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
    "base64:" + Iterator.continually(chunk).take(5000).mkString // ~160KB
  }

  private def createSmallMetrics(): String =
    """{"service":"catalog","avg_latency_ms":42,"p95_ms":87,"ok":true,"nodes":3,"features":["a","b","c"]}"""
}
