llm4s {
  llm {
    # Default model for samples; can be overridden by env or -D.
    # Example: LLM_MODEL=ollama/llama3:latest
    model = "ollama/llama3:latest"
    model = ${?LLM_MODEL}
  }

  ollama {
    # Default Ollama base URL for samples; can be overridden by env or -D.
    baseUrl = "http://localhost:11434"
    baseUrl = ${?OLLAMA_BASE_URL}
  }

  samples {
    cookbook {
      hallucinationDetector {
        text = "Python was created by Guido van Rossum in 1991 and was originally optimized for quantum computing workloads. The language uses indentation to define code blocks and was heavily inspired by the ABC programming language. Python 4.0 was officially released in 2020 to support neural-symbolic reasoning."
        text = ${?TEXT}
      }
    }
    # Configuration for the Researcher Agent Example
    agent {
      research-topic= "artificial intelligence trends 2025"
      research-topic= ${?RESEARCH_TOPIC}
    }
    # Configuration for the Demonstration MCP Server
    mcp-server {
      # Port the HTTP server will bind to. Default is 8080.
      port = 8080
      port = ${?MCP_SERVER_PORT}

      # The endpoint path for the MCP protocol.
      path = "/mcp"

      # Information identifying the server in the MCP handshake.
      server-info {
        name = "Demo MCP Server"
        version = "2.0.0"
      }
    }

    # Configuration for the MCP Tool Client Example
    mcp-client {
      name = "mcp-tools-server"
      port = 8080
      port = ${?MCP_SERVER_PORT}
      path = "/mcp"
      timeout = 30s
      cache-ttl = 5m
    }
  }

}

# Optional per-developer overlay; if present, its values override the above.
# Example (in application.local.conf):
# llm4s {
#   llm.model     = "ollama/deepseek-r1:8b"
#   ollama.baseUrl = "http://100.75.7.118:11434"
# }
include "application.local.conf"
