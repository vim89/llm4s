package org.llm4s.samples.context

import org.llm4s.config.ConfigReader
import org.llm4s.config.ConfigReader.LLMConfig
import org.llm4s.context.{
  ContextManager, 
  ContextConfig, 
  ConversationTokenCounter
}
import org.llm4s.llmconnect.LLM
import org.llm4s.llmconnect.model._
import org.llm4s.types.{ Result, TokenBudget }
import org.slf4j.LoggerFactory

/**
 * Demonstrates the end-to-end context management pipeline using ContextManager.
 *
 * Purpose: Show how the pipeline compacts tool outputs, summarizes history into a digest,
 * optionally squeezes the digest with an LLM, and trims to fit a token budget.
 * Inputs: ConversationFixtures.largeRealistic (~9k tokens) and toolHeavy (~200k tokens).
 * Knobs (from ContextConfig): enableDeterministicCompression, enableLLMCompression,
 * summaryTokenTarget (e.g., 400), headroomPercent (e.g., 8), maxSemanticBlocks, etc.
 *
 * Expected behavior:
 *   â€¢ If conversation > budget:
 *       â€“ ToolDeterministicCompaction runs first (externalize big tool outputs).
 *       â€“ If still over: HistoryCompression creates [HISTORY_SUMMARY] digest(s).
 *       â€“ If enabled and needed: LLM squeeze lowers digest to summaryTokenTarget.
 *       â€“ FinalTokenTrim removes oldest messages (digest is pinned) to fit the window.
 *   â€¢ If conversation â‰¤ budget: no steps run (no-op).
 *   â€¢ Tool-heavy convo collapses mostly via ToolDeterministicCompaction alone.
 *
 * Output:
 *   â€¢ Per-scenario: original â†’ final tokens, budget, steps applied, success flag.
 *   â€¢ Overall: success rate, average compression, step usage counts.
 *
 * To run:
 * ```bash
 * export LLM_MODEL=openai/gpt-4o
 * export OPENAI_API_KEY=sk-your-key-here
 * sbt "samples/runMain org.llm4s.samples.context.ContextPipelineExample"
 * ```
 */

object ContextPipelineExample {
  private val logger = LoggerFactory.getLogger(getClass)

  def main(args: Array[String]): Unit = {
    logger.info("Starting Context Management Pipeline Example")

    val result = for {
      config       <- getConfiguration()
      client       <- createClient(config._2)
      tokenCounter <- createTokenCounter(config._1)
      contextMgr   <- createContextManager(tokenCounter, client)
      results      <- runPipelineDemo(contextMgr, tokenCounter)
    } yield results

    result.fold(
      error => logger.error(s"Example failed: $error"),
      results => {
        logger.info("Example completed successfully")
        displayResults(results)
      }
    )
  }

  private def getConfiguration(): Result[(String, ConfigReader)] = {
    val config = LLMConfig()
    val modelName = config.getOrElse("LLM_MODEL", "openai/gpt-4o")
    logger.info(s"Using model: $modelName")
    Right((modelName, config))
  }

  private def createClient(config: ConfigReader): Result[org.llm4s.llmconnect.LLMClient] = {
    val client = LLM.client(config)
    logger.info("Created LLM client")
    Right(client)
  }

  private def createTokenCounter(modelName: String): Result[ConversationTokenCounter] =
    ConversationTokenCounter.forModel(modelName).map { counter =>
      logger.info(s"Created token counter for: $modelName")
      counter
    }

  private def createContextManager(
    tokenCounter: ConversationTokenCounter,
    client: org.llm4s.llmconnect.LLMClient
  ): Result[ContextManager] =
    ContextManager.create(tokenCounter, ContextConfig.default, Some(client)).map { manager =>
      logger.info("Created context manager with full pipeline enabled")
      manager
    }

  private def runPipelineDemo(
    contextManager: ContextManager,
    tokenCounter: ConversationTokenCounter
  ): Result[PipelineDemoResults] = {
    val conversations = createTestConversations()
    val budgets = Seq(5000, 10000, 20000)
    logger.info(s"Using aggressive budgets to force compression: ${budgets.mkString(", ")} tokens")

    val results = for {
      conversation <- conversations
      budget <- budgets
      result <- testPipelineWithScenario(contextManager, tokenCounter, conversation, budget).toOption
    } yield result

    Right(PipelineDemoResults(results))
  }

  private def testPipelineWithScenario(
    contextManager: ContextManager,
    tokenCounter: ConversationTokenCounter,
    conversation: Conversation,
    budget: TokenBudget
  ): Result[ScenarioResult] = {
    val originalTokens = tokenCounter.countConversation(conversation)
    
    logger.info(s"Testing pipeline: ${conversation.messages.length} messages, $originalTokens tokens â†’ budget $budget")

    contextManager.manageContext(conversation, budget).map { managed =>
      logger.info(s"Pipeline result: ${managed.summary}")
      
      ScenarioResult(
        originalTokens = originalTokens,
        budget = budget,
        finalTokens = managed.finalTokens,
        stepsApplied = managed.stepsApplied.map(_.name),
        compressionRatio = managed.overallCompressionRatio,
        success = managed.finalTokens <= budget
      )
    }
  }

  private def createTestConversations(): Seq[Conversation] = Seq(
    // Scenario 1: Large realistic conversation (forces all compression steps)
    ConversationFixtures.largeRealistic,
    
    // Scenario 2: Tool-heavy conversation (forces tool compression first)
    ConversationFixtures.toolHeavy
  )

  private def displayResults(results: PipelineDemoResults): Unit = {
    logger.info("\nðŸ”„ Context Management Pipeline Example Results")
    logger.info("=" * 60)
    
    results.scenarios.foreach { scenario =>
      logger.info(f"\nðŸ“Š Scenario: ${scenario.originalTokens} â†’ ${scenario.finalTokens} tokens (budget: ${scenario.budget})")
      logger.info(f"   Compression: ${(scenario.compressionRatio * 100).toInt}%% remaining")
      logger.info(f"   Steps applied: ${scenario.stepsApplied.mkString(", ")}")
      
      scenario.success match {
        case true => logger.info("   âœ… Budget achieved")
        case false => logger.info("   âš ï¸  Still over budget")
      }
    }
    
    val successRate = results.scenarios.count(_.success).toDouble / results.scenarios.length * 100
    logger.info(f"\nðŸŽ¯ Overall success rate: $successRate%.1f%% scenarios fit budget")
    
    val avgCompression = results.scenarios.map(_.compressionRatio).sum / results.scenarios.length
    logger.info(f"ðŸ“‰ Average compression ratio: ${(avgCompression * 100).toInt}%%")
    
    val stepsUsage = results.scenarios.flatMap(_.stepsApplied).groupBy(identity).view.mapValues(_.length)
    logger.info("ðŸ”§ Pipeline steps usage:")
    stepsUsage.foreach { case (step, count) => 
      logger.info(f"   $step: $count times")
    }
  }

  case class PipelineDemoResults(scenarios: Seq[ScenarioResult])

  case class ScenarioResult(
    originalTokens: Int,
    budget: TokenBudget,
    finalTokens: Int,
    stepsApplied: Seq[String],
    compressionRatio: Double,
    success: Boolean
  )
}